{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8c1acd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a89671db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset('conllpp')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70fba2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=data['train']\n",
    "validation=data['validation']\n",
    "test=data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad839bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
       " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]\n",
    "# description of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9fa023d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='string', id=None),\n",
       " 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'pos_tags': Sequence(feature=ClassLabel(names=['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB'], id=None), length=-1, id=None),\n",
       " 'chunk_tags': Sequence(feature=ClassLabel(names=['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP'], id=None), length=-1, id=None),\n",
       " 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "189ca332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '1', 'tokens': ['Peter', 'Blackburn', '0148468412'], 'pos_tags': [22, 22], 'chunk_tags': [11, 12], 'ner_tags': [1, 2, 9]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import random\n",
    "# Define the new token and NER tag to add\n",
    "\n",
    "new_ner_tag = 9\n",
    "\n",
    "# Create a new list to store the modified rows\n",
    "modified_rows = []\n",
    "\n",
    "# Loop through each row in the dataset and modify the 'tokens' and 'ner_tags' columns\n",
    "for row in train:\n",
    "    new_token = \"0\"+''.join(random.choices('123456789', k=9))\n",
    "    modified_row = {\n",
    "        'id': row['id'],\n",
    "        'tokens': row['tokens'] + [new_token],\n",
    "        'pos_tags': row['pos_tags'],\n",
    "        'chunk_tags': row['chunk_tags'],\n",
    "        'ner_tags': row['ner_tags'] + [new_ner_tag]\n",
    "    }\n",
    "    modified_rows.append(modified_row)\n",
    "\n",
    "# Create the new dataset 'train_final' with the modified rows\n",
    "train_final = Dataset.from_dict({\n",
    "    'id': [row['id'] for row in modified_rows],\n",
    "    'tokens': [row['tokens'] for row in modified_rows],\n",
    "    'pos_tags': [row['pos_tags'] for row in modified_rows],\n",
    "    'chunk_tags': [row['chunk_tags'] for row in modified_rows],\n",
    "    'ner_tags': [row['ner_tags'] for row in modified_rows],\n",
    "})\n",
    "\n",
    "# Check the first row of the new dataset 'train_final'\n",
    "print(train_final[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0886946d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '1', 'tokens': ['LONDON', '1996-08-30', '0162583319'], 'pos_tags': [22, 11], 'chunk_tags': [11, 12], 'ner_tags': [5, 0, 9]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import random\n",
    "# Define the new token and NER tag to add\n",
    "\n",
    "new_ner_tag = 9\n",
    "\n",
    "# Create a new list to store the modified rows\n",
    "modified_rows = []\n",
    "\n",
    "# Loop through each row in the dataset and modify the 'tokens' and 'ner_tags' columns\n",
    "for row in validation:\n",
    "    new_token = \"0\"+''.join(random.choices('123456789', k=9))\n",
    "    modified_row = {\n",
    "        'id': row['id'],\n",
    "        'tokens': row['tokens'] + [new_token],\n",
    "        'pos_tags': row['pos_tags'],\n",
    "        'chunk_tags': row['chunk_tags'],\n",
    "        'ner_tags': row['ner_tags'] + [new_ner_tag]\n",
    "    }\n",
    "    modified_rows.append(modified_row)\n",
    "\n",
    "# Create the new dataset 'train_final' with the modified rows\n",
    "validation_final = Dataset.from_dict({\n",
    "    'id': [row['id'] for row in modified_rows],\n",
    "    'tokens': [row['tokens'] for row in modified_rows],\n",
    "    'pos_tags': [row['pos_tags'] for row in modified_rows],\n",
    "    'chunk_tags': [row['chunk_tags'] for row in modified_rows],\n",
    "    'ner_tags': [row['ner_tags'] for row in modified_rows],\n",
    "})\n",
    "\n",
    "# Check the first row of the new dataset 'train_final'\n",
    "print(validation_final[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b05177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_final['ner_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cd3e619",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                         1\n",
       "tokens        [Peter, Blackburn, 0148468412]\n",
       "pos_tags                            [22, 22]\n",
       "chunk_tags                          [11, 12]\n",
       "ner_tags                           [1, 2, 9]\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(train_final).iloc[1]\n",
    "# structure of single data row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70f2410b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC', 'B-PHONE', 'I-PHONE'], id=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = data['train'].features['ner_tags'].feature\n",
    "tags.names.append(\"B-PHONE\")\n",
    "tags.names.append(\"I-PHONE\")\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d7ec3f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-PER',\n",
       " 2: 'I-PER',\n",
       " 3: 'B-ORG',\n",
       " 4: 'I-ORG',\n",
       " 5: 'B-LOC',\n",
       " 6: 'I-LOC',\n",
       " 7: 'B-MISC',\n",
       " 8: 'I-MISC',\n",
       " 9: 'B-PHONE',\n",
       " 10: 'I-PHONE'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get tag names according to index values in array\n",
    "\n",
    "index2tag = {idx:tag for idx, tag in enumerate(tags.names)}\n",
    "tag2index = {tag:idx for idx, tag in enumerate(tags.names)}\n",
    "\n",
    "\n",
    "index2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7ba2e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index2tag[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acc74491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping tag names 'B-PER'... to rows\n",
    "def create_tag_names(batch):\n",
    "  tag_name = {'ner_tags_str': [index2tag[idx] for idx in batch['ner_tags']]}\n",
    "  return tag_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8a4e2ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "794a182b5d974832a678cfcc380a8913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_final = train_final.map(create_tag_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6e34b4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokens          [Peter, Blackburn, 0148468412]\n",
       "ner_tags                             [1, 2, 9]\n",
       "ner_tags_str           [B-PER, I-PER, B-PHONE]\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(train_final[:])[['tokens', 'ner_tags','ner_tags_str']].iloc[1]\n",
    "# structure of single data row after mapping tag names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba08fa5",
   "metadata": {},
   "source": [
    "# Tokanization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9220538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"distilbert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f9b947b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimized tokenizer or not // fast-optimized\n",
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c5c6e82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'The', 'European', 'Commission', 'said', 'on', 'Thursday', 'it', 'disagreed', 'with', 'German', 'advice', 'to', 'consumers', 'to', 's', '##hun', 'British', 'la', '##mb', 'until', 'scientists', 'determine', 'whether', 'mad', 'cow', 'disease', 'can', 'be', 'transmitted', 'to', 'sheep', '.', '03', '##14', '##6', '##17', '##8', '##26', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "input_row=train_final[3]\n",
    "inputs = input_row['tokens']\n",
    "\n",
    "# split to tokens. True means already splitted words\n",
    "\n",
    "inputs = tokenizer(inputs, is_split_into_words=True)\n",
    "print(inputs.tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b53f72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'European', 'Commission', 'said', 'on', 'Thursday', 'it', 'disagreed', 'with', 'German', 'advice', 'to', 'consumers', 'to', 'shun', 'British', 'lamb', 'until', 'scientists', 'determine', 'whether', 'mad', 'cow', 'disease', 'can', 'be', 'transmitted', 'to', 'sheep', '.', '0314617826']\n",
      "['O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PHONE']\n"
     ]
    }
   ],
   "source": [
    "print(input_row['tokens'])\n",
    "print(input_row['ner_tags_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9dd5e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "  new_labels = []\n",
    "  current_word=None\n",
    "  for word_id in word_ids:\n",
    "    if word_id != current_word:\n",
    "      current_word = word_id\n",
    "      label = -100 if word_id is None else labels[word_id]\n",
    "      new_labels.append(label)\n",
    "\n",
    "    elif word_id is None:\n",
    "      new_labels.append(-100)\n",
    "\n",
    "    else:\n",
    "      label = labels[word_id]\n",
    "      if label%2==1:\n",
    "        label = label + 1\n",
    "      new_labels.append(label)\n",
    "\n",
    "  return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "435fe6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9] [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 14, 15, 16, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 30, 30, 30, 30, 30, None]\n"
     ]
    }
   ],
   "source": [
    "labels = input_row['ner_tags']\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels, word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c54dfca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get labels that have target ners according to index2tag\n",
    "# align_labels_with_tokens(labels, word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f1e9740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-PER',\n",
       " 2: 'I-PER',\n",
       " 3: 'B-ORG',\n",
       " 4: 'I-ORG',\n",
       " 5: 'B-LOC',\n",
       " 6: 'I-LOC',\n",
       " 7: 'B-MISC',\n",
       " 8: 'I-MISC',\n",
       " 9: 'B-PHONE',\n",
       " 10: 'I-PHONE'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33b89346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to tokens. True means already splitted words and get labels that have target full dataset\n",
    "def tokenize_and_align_labels(input_data):\n",
    "#   print(input_data['ner_tags'])\n",
    "  tokenized_inputs = tokenizer(input_data['tokens'], truncation=True, is_split_into_words=True)\n",
    "\n",
    "  all_labels = input_data['ner_tags']\n",
    "\n",
    "  new_labels = []\n",
    "  for i, labels in enumerate(all_labels):\n",
    "    word_ids = tokenized_inputs.word_ids(i)\n",
    "    new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "  tokenized_inputs['labels'] = new_labels\n",
    "\n",
    "  return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0acc42e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4271284415df4d50bab18c7bfd8a4f19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626b177321c24a2880b08b0cad5cbc40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets_train = train_final.map(tokenize_and_align_labels, batched=True, remove_columns=data['train'].column_names)\n",
    "tokenized_datasets_validation = validation_final.map(tokenize_and_align_labels, batched=True, remove_columns=data['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee950bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ner_tags_str', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 14041\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# structure of data after tokenaization\n",
    "tokenized_datasets_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060e8e21",
   "metadata": {},
   "source": [
    "# Data Collation and matrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "37d75db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c6c6bbab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ec5fc230",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load('seqeval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "8a0e6a5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-PER',\n",
       " 'I-PER',\n",
       " 'B-ORG',\n",
       " 'I-ORG',\n",
       " 'B-LOC',\n",
       " 'I-LOC',\n",
       " 'B-MISC',\n",
       " 'I-MISC',\n",
       " 'B-PHONE',\n",
       " 'I-PHONE']"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = data['train'].features['ner_tags'].feature.names\n",
    "label_names.append(\"B-PHONE\")\n",
    "label_names.append(\"I-PHONE\")\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "6ce29ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# evelution metric\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "  logits, labels = eval_preds\n",
    "\n",
    "  predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "  true_labels = [[label_names[l] for l in label if l!=-100] for label in labels]\n",
    "\n",
    "  true_predictions = [[label_names[p] for p,l in zip(prediction, label) if l!=-100]\n",
    "                      for prediction, label in zip(predictions, labels)]\n",
    "\n",
    "  all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "  return {\"precision\": all_metrics['overall_precision'],\n",
    "          \"recall\": all_metrics['overall_recall'],\n",
    "          \"f1\": all_metrics['overall_f1'],\n",
    "          \"accuracy\": all_metrics['overall_accuracy']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fdb594",
   "metadata": {},
   "source": [
    "# Model Traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "8a58ba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as index2tag bothways\n",
    "id2label = {i:label for i, label in enumerate(label_names)}\n",
    "label2id = {label:i for i, label in enumerate(label_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "932c292e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC', 7: 'B-MISC', 8: 'I-MISC', 9: 'B-PHONE', 10: 'I-PHONE'}\n"
     ]
    }
   ],
   "source": [
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "aa88f550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "                                                    model_checkpoint,\n",
    "                                                    id2label=id2label,\n",
    "                                                    label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "4d955d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "62668408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set tranining arguments-\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\"distilbert-finetuned-ner\",\n",
    "                         evaluation_strategy=\"steps\",\n",
    "                         eval_steps=400,\n",
    "                         save_strategy=\"epoch\",\n",
    "                         per_device_train_batch_size=16,\n",
    "                         learning_rate = 2e-5,\n",
    "                         num_train_epochs=1,\n",
    "                         weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "96db1da3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='878' max='878' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [878/878 1:07:29, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.072447</td>\n",
       "      <td>0.907820</td>\n",
       "      <td>0.925696</td>\n",
       "      <td>0.916671</td>\n",
       "      <td>0.977899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.086700</td>\n",
       "      <td>0.062198</td>\n",
       "      <td>0.921790</td>\n",
       "      <td>0.941144</td>\n",
       "      <td>0.931367</td>\n",
       "      <td>0.981581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=878, training_loss=0.07847619979962672, metrics={'train_runtime': 4062.0702, 'train_samples_per_second': 3.457, 'train_steps_per_second': 0.216, 'total_flos': 199330977062172.0, 'train_loss': 0.07847619979962672, 'epoch': 1.0})"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "from transformers import Trainer\n",
    "trainer = Trainer(model=model,\n",
    "                  args=args,\n",
    "                  train_dataset = tokenized_datasets_train,\n",
    "                  eval_dataset = tokenized_datasets_validation,\n",
    "                  data_collator=data_collator,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  tokenizer=tokenizer)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "ab4873ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('/Desktop/modified_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71d2b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model('/Desktop/modified')- saved before I-Phone,B-Phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc5cf87b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9934889,\n",
       "  'word': 'Nimeth Nimdinu',\n",
       "  'start': 11,\n",
       "  'end': 25},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9910805,\n",
       "  'word': 'KGP Talkie',\n",
       "  'start': 37,\n",
       "  'end': 47},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.86119616,\n",
       "  'word': 'rathnapura',\n",
       "  'start': 60,\n",
       "  'end': 70},\n",
       " {'entity_group': 'PHONE',\n",
       "  'score': 0.99916524,\n",
       "  'word': '9999999999999',\n",
       "  'start': 87,\n",
       "  'end': 100}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing custom text\n",
    "from transformers import pipeline\n",
    "\n",
    "checkpoint = \"/Desktop/modified_2\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=checkpoint, aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "token_classifier(\"My name is Nimeth Nimdinu. I work at KGP Talkie and live in rathnapura,phone number is 9999999999999\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea71b592",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9512ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
